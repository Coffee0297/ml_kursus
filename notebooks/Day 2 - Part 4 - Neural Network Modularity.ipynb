{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lær de neurale netværk bedre at kende\n",
    "Denne Notebook skal vise jer, hvordan man kan opbygge forskellige neurale netværk fra bunden med PyTorch.   \n",
    "Indtil videre har i brugt modeller som jeg har kodet for jer, hvor I nemt har kunne ændre parametrene.  \n",
    "\n",
    "  \n",
    "\n",
    "Nu skal I selv prøve...  \n",
    "  \n",
    "## Eksempel på en PyTorch Model Class\n",
    "```python\n",
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self, ...):\n",
    "        super().__init__()\n",
    "        ...\n",
    "\n",
    "    def forward(self, x):\n",
    "        ...\n",
    "        return out\n",
    "```\n",
    "\n",
    "I `__init__` funktionen kan vi initialisere modellens parametre og opbygge dens arkitektur.    \n",
    "I dette eksempel bygger vi en *Multilayer Perceptron*.    \n",
    "\n",
    "\n",
    "Netværket har behov for at vide, hvor mange inputs (antal datapunkter i form af kolonner) der er i vores data.\n",
    "Derfor gemmer vi dette i `self.input_size`.  \n",
    "  \n",
    "  \n",
    "Netværket har også behov for at vide hvilken non-lineær function vi vil anvende.  \n",
    "Funktionerne er implementeret i PyTorch her: https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity\n",
    "Eksempelvis kan vi vælge `Rectified Linear Unit (ReLU)`, som er en typisk anvendt non-lineær funktion.   \n",
    "Denne gemmer vi i `self.activation_fnc`\n",
    "```python\n",
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.activation = nn.ReLU\n",
    "        ...\n",
    "```  \n",
    "  \n",
    "Vi skal bestemme størrelserne på lagene ved at bruge et helt tal som matcher tallet i det næste lag.  \n",
    "Herfra kan vi begynde at opbygge de forskellige lag i netværket.  \n",
    "1. Flatten layer - Vi flader dimensionerne ud. Dette er kun nødvendigt i et *fully connected network*\n",
    "2. Input layer - Det første lag i netværket.\n",
    "3. Hidden layer(s) - Et eller flere *hidden layers* som skiftevis er linæere og non-lineære\n",
    "4. Output layer - Det sidste lag som bestemmer hvor mange output vi skal have.\n",
    "\n",
    "```python\n",
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.activation = nn.ReLU\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        self.input_layer = nn.Linear(self.input_size, 64)\n",
    "        \n",
    "        self.hidden_layer_1 = nn.Linear(64, 128)\n",
    "        self.hidden_layer_2 = nn.Linear(128, 128)\n",
    "        self.hidden_block = nn.Sequential(\n",
    "            [\n",
    "                self.hidden_layer_1,\n",
    "                self.activation(),\n",
    "                self.hidden_layer_2,\n",
    "                self.activation()\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        self.output_layer = nn.Linear(128, 24)\n",
    "\n",
    "```  \n",
    "  \n",
    "  \n",
    "Til sidst skal vi bestemme hvordan vores data skal \"flyde\" gennem netværket. Dette gør vi med `forward` funktionen i vores `Model` class.\n",
    "  \n",
    "```python\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.input_layer(x)\n",
    "        x = self.hidden_block(x)\n",
    "        out = self.output_layer(x)\n",
    "        return out\n",
    "```\n",
    "\n",
    "**Prøv selv!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.activation = nn.ReLU\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        self.input_layer = nn.Sequential(nn.Linear(self.input_size, 64), self.activation())\n",
    "        \n",
    "        self.hidden_layer_1 = nn.Linear(64, 128)\n",
    "        self.hidden_layer_2 = nn.Linear(128, 128)\n",
    "        self.hidden_block = nn.Sequential(\n",
    "                self.hidden_layer_1,\n",
    "                self.activation(),\n",
    "                self.hidden_layer_2,\n",
    "                self.activation()\n",
    "        )\n",
    "        \n",
    "        self.output_layer = nn.Linear(128, 24)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.input_layer(x)\n",
    "        x = self.hidden_block(x)\n",
    "        out = self.output_layer(x)\n",
    "        return out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
